# Implementaci√≥n: Cap√≠tulo 2 - Trabajando con Datos de Texto

Este repositorio contiene la implementaci√≥n pr√°ctica y explicada de los conceptos del **Cap√≠tulo 2** del libro *"Build a Large Language Model (From Scratch)"* de Sebastian Raschka.

El objetivo principal es preparar datos de texto para el entrenamiento de un LLM tipo GPT, cubriendo desde la tokenizaci√≥n hasta la generaci√≥n de embeddings.

## üìÇ Archivos del Proyecto

*   **`embeddings.ipynb`**: Notebook principal creado. Contiene:
    *   C√≥digo core para tokenizaci√≥n (BPE) y creaci√≥n de datasets.
    *   Explicaciones te√≥ricas sobre por qu√© dividimos el texto, usamos *sliding windows* y qu√© representan los embeddings.
    *   Resultados del experimento de *overlap*.
*   **`experiment.py`**: Script de Python utilizado para realizar experimentos cuantitativos sobre el impacto del *stride* en la cantidad de muestras de entrenamiento.
*   **`ch02.ipynb`**: Notebook original de referencia del libro.
*   **`the-verdict.txt`**: Corpus de texto utilizado (cuento corto de Edith Wharton).
*   **`the veredict.txt`**: (Archivo original renombrado a `the-verdict.txt` para consistencia).

## üöÄ Proceso de Implementaci√≥n

### 1. Preparaci√≥n del Entorno
Se descargaron los archivos necesarios y se renombr√≥ el archivo de texto a `the-verdict.txt` para asegurar la compatibilidad con el c√≥digo est√°ndar del libro.

### 2. Tokenizaci√≥n (Byte Pair Encoding)
Implementamos el tokenizador BPE utilizando la librer√≠a `tiktoken` (modelo `gpt2`). BPE es crucial porque permite manejar palabras fuera del vocabulario dividi√©ndolas en subpalabras, equilibrando el tama√±o del vocabulario y la longitud de la secuencia.

### 3. Dataset y Sliding Window
Creamos la clase `GPTDatasetV1` y un `DataLoader` que utiliza una ventana deslizante (*sliding window*) para generar pares de `(input, target)` para el entrenamiento autoregresivo.

### 4. Experimento: Impacto del Stride
Utilizamos el script `experiment.py` para analizar c√≥mo el par√°metro `stride` (paso de la ventana) afecta la cantidad de datos generados.

**Resultados (para `max_length=4`):**
*   **Stride 4 (Sin solapamiento):** 1286 muestras.
*   **Stride 1 (Alto solapamiento):** 5141 muestras.

**Conclusi√≥n:** Un `stride` menor (mayor solapamiento) act√∫a como una forma de *data augmentation* natural, generando casi 4 veces m√°s datos de entrenamiento del mismo corpus, lo cual es vital para evitar overfitting en datasets peque√±os.

### 5. Embeddings
Finalmente, implementamos capas de embeddings (`torch.nn.Embedding`) para transformar los IDs de los tokens en vectores densos continuos, sumando embeddings posicionales para retener la informaci√≥n del orden de las palabras.

## üõ†Ô∏è C√≥mo Ejecutar

1.  **Instalar dependencias:**
    ```bash
    pip install torch tiktoken
    ```

2.  **Ejecutar el Notebook:**
    Abrir `embeddings.ipynb` en Jupyter Lab o VS Code y ejecutar las celdas secuencialmente.

3.  **Reproducir el Experimento:**
    EJecutar el script desde la terminal:
    ```bash
    python experiment.py
    ```

## üß† Conceptos Clave para Agentes
*   **Embeddings:** Son la base de la "comprensi√≥n" sem√°ntica. Permiten que un agente relacione instrucciones de usuario con conceptos aprendidos.
*   **Context Window:** La gesti√≥n eficiente de la ventana de contexto (v√≠a sliding windows durante el entrenamiento) permite al agente mantener la coherencia en conversaciones largas.
