{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data and Embeddings\n",
    "\n",
    "This notebook covers the essential steps to prepare text for a Large Language Model (LLM) as described in Chapter 2 of \"Build a Large Language Model (From Scratch)\". We go through tokenization, data sampling, and creating embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, we load the raw text from \"The Verdict\" by Edith Wharton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization with BPE\n",
    "\n",
    "### Why Tokenization Matters for LLMs?\n",
    "\n",
    "LLMs cannot process raw strings directly; they require numerical input. Tokenization is the bridge between human language and machine-readable numbers. It breaks down text into smaller units (tokens). \n",
    "\n",
    "We use **Byte Pair Encoding (BPE)** (via `tiktoken`) because it efficiently handles out-of-vocabulary words by breaking them into subword units, balancing vocabulary size and sequence length. This is crucial for agentic systems to robustly handle diverse user inputs without failing on unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BPE tokenizer (GPT-2 version)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(\"Encoded IDs:\", integers)\n",
    "print(\"Decoded:\", tokenizer.decode(integers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Sampling with Sliding Window\n",
    "\n",
    "### Why Use a Sliding Window?\n",
    "\n",
    "LLMs are trained autoregressively to predict the *next* token given a context. To train them efficiently, we need many examples of `(input, target)` pairs. \n",
    "\n",
    "A sliding window approach allows us to generate multiple training examples from a single text by moving a window of fixed size (`max_length`) across the text. The `stride` determines how much the window moves. This maximizes the data efficiency and teaches the model to handle contexts shifting over time, which is fundamental for maintaining coherence in long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    return dataloader, len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment: Impact of Stride on Sample Count\n",
    "\n",
    "We investigate how changing the `stride` affects the number of training samples generated. We use a small `max_length=4` for demonstration.\n",
    "\n",
    "### Experiment Results\n",
    "\n",
    "| Experiment | max_length | stride | Samples Generated |\n",
    "|------------|------------|--------|-------------------|\n",
    "| 1 (No Overlap) | 4 | 4 | **1286** |\n",
    "| 2 (High Overlap) | 4 | 1 | **5141** |\n",
    "| 3 (Mid Overlap) | 4 | 2 | **2571** |\n",
    "\n",
    "### Conclusion on Overlap\n",
    "Using a smaller stride (e.g., `stride=1`) results in significantly more training samples (nearly 4x more than no overlap). \n",
    "\n",
    "**Why is overlap useful?**\n",
    "1.  **Data Augmentation:** It creates more training data solely from the existing corpus, allowing the model to see the same tokens in slightly different contexts. This helps prevent overfitting on small datasets.\n",
    "2.  **Context Continuity:** It ensures the model learns to predict tokens regardless of their exact absolute position in a fixed window. It smooths out the learning boundaries that would exist if we only used non-overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to generate experiment results (run in experiment.py)\n",
    "# dl, n_samples = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "# print(len(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embeddings\n",
    "\n",
    "### Why do embeddings encode meaning, and how are they related to NN concepts?\n",
    "\n",
    "Embeddings are dense vector representations of tokens. Unlike sparse one-hot encodings (which are high-dimensional and orthogonal), embeddings exist in a continuous lower-dimensional space where \"meaning\" is encoded as distance and direction.\n",
    "\n",
    "**Relation to NN Concepts:**\n",
    "An embedding layer is technically just a **linear layer** (fully connected layer) without a bias term, applied to a one-hot encoded input. \n",
    "- In `torch.nn.Embedding`, we simply look up the row corresponding to the token ID. \n",
    "- This lookup is mathematically equivalent to multiplying a one-hot vector with a weight matrix $W$. \n",
    "- These weights are **learnable parameters**. During backpropagation, the network adjusts these vectors so that words appearing in similar contexts (e.g., \"cat\" and \"dog\") end up having vectors that are geometrically close (high cosine similarity).\n",
    "\n",
    "For agents, good embeddings are the foundation of \"understanding\" user intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "context_len = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_len, output_dim)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader, _ = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Input IDs shape:\", inputs.shape)\n",
    "\n",
    "# Token Embeddings\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token Embeddings shape:\", token_embeddings.shape)\n",
    "\n",
    "# Positional Embeddings\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(4))\n",
    "print(\"Positional Embeddings shape:\", pos_embeddings.shape)\n",
    "\n",
    "# Final Input Embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Final Input Embeddings shape:\", input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
